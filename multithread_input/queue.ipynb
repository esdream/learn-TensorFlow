{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow数据读取机制中内存队列，文件名队列详解见[十图详解tensorflow数据读取机制](https://zhuanlan.zhihu.com/p/27238630)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多线程与queue（内存队列）\n",
    "Tensorflow中提供了FIFOQueue和RandomShuffleQueue两种内存队列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "1\n",
      "11\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 创建一个先进先出队列，指定队列中最多可以保存两个元素，并指定类型\n",
    "q = tf.FIFOQueue(2, dtypes='int32')\n",
    "\n",
    "# 创建一个随机队列，它会将队列中元素打乱，每次出队操作得到的是从当前队列\n",
    "# 所有元素中随机挑选的一个，必须传入min_after_dequeue（队列中最少元素个数）参数\n",
    "# q = tf.RandomShuffleQueue(2, min_after_dequeue=0, dtypes='int32')\n",
    "\n",
    "# 使用enqueue_many初始化函数，使用队列之前必须明确调用初始化\n",
    "init = q.enqueue_many(([0, 10], ))\n",
    "\n",
    "# 使用dequeue函数出队\n",
    "x = q.dequeue()\n",
    "\n",
    "y = x + 1\n",
    "\n",
    "# 重新入队\n",
    "q_inc = q.enqueue([y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 运行初始化队列操作\n",
    "    init.run()\n",
    "    for _ in range(5):\n",
    "        v, _  = sess.run([x, q_inc])\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow提供了**tf.Coordinator**和**tf.QueueRunner**两个类来完成多线程协同的功能。**tf.Coordinator**主要用于协同多个线程一起停止，并提供了**should_stop**，**request_stop**和**join**三个函数。\n",
    "\n",
    "启动线程之前，需要声明一个`tf.Coordinator`类，并将这个类传入每一个创建的建成中。启动的线程需要一直查询`tf.Coordinator`类中提供的`should_stop`函数，当这个函数的返回值为True时，当前线程退出。每一个启动的线程都可以通过调用`request_stop`函数通知其他线程退出。\n",
    "\n",
    "当一个线程调用`request_stop`函数后，`should_stop`函数的返回值被设置为True。这样其他线程就可以同时终止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on id: 0\n",
      "Working on id: 1\n",
      "Working on id: 2\n",
      "\n",
      "\n",
      "\n",
      "Working on id: 3\n",
      "\n",
      "Working on id: 4\n",
      "\n",
      "Working on id: 0\n",
      "\n",
      "Working on id: 1\n",
      "\n",
      "Working on id: 2\n",
      "\n",
      "Working on id: 3\n",
      "\n",
      "Working on id: 4\n",
      "\n",
      "Working on id: 0\n",
      "\n",
      "Working on id: 1\n",
      "\n",
      "Working on id: 2\n",
      "\n",
      "Stoping from id: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# 线程中运行的程序，该程序每隔1s判断是否需要停止并打印自己的ID\n",
    "def my_loop(coord, worker_id):\n",
    "    # 使用tf.Coordinator类提供的协同工具判断当前线程是否需要停止\n",
    "    while not coord.should_stop():\n",
    "        # 随机停止所有线程\n",
    "        if(np.random.rand() < 0.1):\n",
    "            print('Stoping from id: %d\\n' % worker_id)\n",
    "            # 调用coord.request_stop()通知其他线程停止\n",
    "            coord.request_stop()\n",
    "        else:\n",
    "            # 打印当前线程id\n",
    "            print('Working on id: %d\\n' % worker_id)\n",
    "        time.sleep(1)\n",
    "        \n",
    "# 声明一个tf.train.Coordinator类协同多个线程\n",
    "coord = tf.train.Coordinator()\n",
    "# 创建5个线程\n",
    "threads = [threading.Thread(target=my_loop, args=(coord, i)) for i in range(5)]\n",
    "# 启动所有线程\n",
    "for t in threads:\n",
    "    t.start()\n",
    "\n",
    "# 等待所有线程退出\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.QueueRunner**用于启动多个线程操作同一个队列，启动的这些线程可以通过`tf.Coordinator`统一管理。\n",
    "\n",
    "以下程序利用多线程将随机数入队，单线程出队打印。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.480941\n",
      "0.834668\n",
      "-1.05499\n",
      "-1.0778\n",
      "-1.31522\n",
      "0.102408\n",
      "-0.166196\n",
      "-1.63906\n",
      "0.400012\n",
      "-0.207954\n",
      "0.814503\n",
      "1.36503\n",
      "0.305135\n",
      "1.4215\n",
      "1.58674\n",
      "-0.802489\n",
      "1.56601\n",
      "-0.0561557\n",
      "0.591244\n",
      "-0.826584\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "queue = tf.FIFOQueue(100, 'float')\n",
    "# 定义队列的入队操作，tf.random_normal()从正态分布输出随机值\n",
    "enqueue_op = queue.enqueue([tf.random_normal([1])])\n",
    "\n",
    "# 创建多个线程运行队列的入队操作，queue是被操作的队列，[enqueue_op] * 5表示启动5个\n",
    "# 线程，每个线程中运行的是enqueue_op操作\n",
    "qr = tf.train.QueueRunner(queue, [enqueue_op] * 5)\n",
    "\n",
    "# 将定义过的QueueRunner加入Tensorflow计算图上指定的集合\n",
    "# tf.train.add_queue_runner函数没有指定集合\n",
    "# 则加入默认集合tf.GraphKeys.QUEUE_RUNNERS\n",
    "tf.train.add_queue_runner(qr)\n",
    "# 定义出队操作\n",
    "out_tensor = queue.dequeue()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 使用tf.train.Coordinator协同启动的线程\n",
    "    coord = tf.train.Coordinator()\n",
    "    # 使用tf.train.QueueRunner时，需要明确调用tf.train.start_queue_runners\n",
    "    # 启动所有线程。tf.train.start_queue_runners函数默认启动tf.GraphKeys.QUEUE_RUNNERS\n",
    "    # 集合中所有的QueueRunner。\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    # 获取队列中的取值\n",
    "    for _ in range(20):\n",
    "        print(sess.run(out_tensor)[0])\n",
    "    \n",
    "    # 读取完值后，使用tf.train.Coordinator停止所有线程\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入文件名队列\n",
    "**tf.train.string_input_producer**函数会使用初始化时提供的文件列表创建一个**文件名队列**。\n",
    "\n",
    "通过设置**shuffle**参数，可以打乱文件名队列中的顺序。当`shuffle`参数为True时，文件在加入文件名队列之前就会被打乱顺序。`tf.train.string_input_producer`生成的文件名队列可以同时被多个文件读取线程操作，而且会将队列中的文件均匀地分给不同线程。\n",
    "\n",
    "通过设置**num_epochs**参数限制文件名队列加载文件列表的最大轮数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "data_path = os.path.join('.', 'data')\n",
    "if(not os.path.exists(data_path)):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "# 模拟海量数据情况下将数据写入不同的文件。num_shards定义了总共写入多少个TFRecords文件。\n",
    "# instatnces_per_shard定义了每个TFRcords文件中有多少个数据。\n",
    "num_shards = 2\n",
    "instances_per_shard = 2\n",
    "\n",
    "for i in range(num_shards):\n",
    "    # 将数据分为多个文件时，可以将不同文件以类似0000n-of-0000m的前缀区分，m表示TFRecords数据\n",
    "    # 总个数（2个），n表示第几个TFRecords数据\n",
    "    filename = os.path.join(data_path, 'data_000%d_of_000%d.tfrecords' % (i, num_shards))\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    # 将数据封装为Example结构写入TFRecord文件。这里instances_per_shard=2，意味着每个TFRecords中\n",
    "    # example的数量为2。第一个example中'i'的值为i,'j'的值为0，第二个example中'i'的值为i，\n",
    "    # 'j'的值为1 \n",
    "    for j in range(instances_per_shard):\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'i': tf.train.Feature(int64_list=tf.train.Int64List(value=[i])),\n",
    "            'j': tf.train.Feature(int64_list=tf.train.Int64List(value=[j]))\n",
    "        }))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`tf.train.match_filenames_once`和`tf.train.string_input_producer`读取TFRecrods中的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[0, 1]\n",
      "[1, 0]\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "data_path = os.path.join('.', 'data')\n",
    "\n",
    "# 要读取train.match_filenames_once中的结果，在run之前需要先调用\n",
    "# global_variables_initializer()和local_variables_initializer()初始化。\n",
    "# train.match_filenames_once返回一个文件名列表，其中每个元素是bytes格式的文件名\n",
    "filenames = tf.train.match_filenames_once(os.path.join(data_path, '*'))\n",
    "\n",
    "# 如果不给定num_epochs参数值，则会一直生成下去（类似于python generator）\n",
    "filename_queue = tf.train.string_input_producer(filenames, shuffle=False)\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "        'i': tf.FixedLenFeature([], tf.int64),\n",
    "        'j': tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    ")\n",
    "\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "#     print(sess.run(filenames))\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    # 多次执行获取数据的操作\n",
    "    for _ in range(4):\n",
    "        print(sess.run([features['i'], features['j']]))\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组合训练数据（batching）\n",
    "Tensorflow提供了**tf.train.batch**和**tf.train.shuffle_batch**函数将单个样例组织成batch形式输出。这两个函数会生成一个**队列**，队列的入队操作是**生成单个sample**的方法，而每次出队得到的是一个**batch**样例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.FailedPreconditionError'>, Attempting to use uninitialized value input_producer/limit_epochs/epochs\n",
      "\t [[Node: input_producer/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer/limit_epochs/epochs\"], limit=5, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer/limit_epochs/epochs)]]\n",
      "\n",
      "Caused by op 'input_producer/limit_epochs/CountUpTo', defined at:\n",
      "  File \"D:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"D:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-b54633b03d75>\", line 12, in <module>\n",
      "    filename_queue = tf.train.string_input_producer(filenames, shuffle=True, num_epochs=5)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py\", line 232, in string_input_producer\n",
      "    cancel_op=cancel_op)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py\", line 158, in input_producer\n",
      "    input_tensor = limit_epochs(input_tensor, num_epochs)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py\", line 98, in limit_epochs\n",
      "    counter = epochs.count_up_to(num_epochs)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 589, in count_up_to\n",
      "    return state_ops.count_up_to(self._variable, limit=limit)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 126, in count_up_to\n",
      "    result = _op_def_lib.apply_op(\"CountUpTo\", ref=ref, limit=limit, name=name)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "FailedPreconditionError (see above for traceback): Attempting to use uninitialized value input_producer/limit_epochs/epochs\n",
      "\t [[Node: input_producer/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer/limit_epochs/epochs\"], limit=5, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer/limit_epochs/epochs)]]\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "FIFOQueue '_122_batch/fifo_queue' is closed and has insufficient elements (requested 3, current size 0)\n\t [[Node: batch = QueueDequeueManyV2[component_types=[DT_INT64, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\n\nCaused by op 'batch', defined at:\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-72837be8a0f3>\", line 10, in <module>\n    example_batch, label_batch = tf.train.batch([example, label], batch_size=batch_size, capacity=capacity)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py\", line 917, in batch\n    name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py\", line 712, in _batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\", line 458, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 1328, in _queue_dequeue_many_v2\n    timeout_ms=timeout_ms, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): FIFOQueue '_122_batch/fifo_queue' is closed and has insufficient elements (requested 3, current size 0)\n\t [[Node: batch = QueueDequeueManyV2[component_types=[DT_INT64, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfRangeError\u001b[0m: FIFOQueue '_122_batch/fifo_queue' is closed and has insufficient elements (requested 3, current size 0)\n\t [[Node: batch = QueueDequeueManyV2[component_types=[DT_INT64, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-72837be8a0f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mcur_example_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_label_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_example_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_label_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfRangeError\u001b[0m: FIFOQueue '_122_batch/fifo_queue' is closed and has insufficient elements (requested 3, current size 0)\n\t [[Node: batch = QueueDequeueManyV2[component_types=[DT_INT64, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\n\nCaused by op 'batch', defined at:\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-72837be8a0f3>\", line 10, in <module>\n    example_batch, label_batch = tf.train.batch([example, label], batch_size=batch_size, capacity=capacity)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py\", line 917, in batch\n    name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py\", line 712, in _batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\", line 458, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 1328, in _queue_dequeue_many_v2\n    timeout_ms=timeout_ms, name=name)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): FIFOQueue '_122_batch/fifo_queue' is closed and has insufficient elements (requested 3, current size 0)\n\t [[Node: batch = QueueDequeueManyV2[component_types=[DT_INT64, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 使用上一节中的Example读取到的数据。假设i表示一个sample的特征向量，\n",
    "# 比如一张图像的像素矩阵；j表示该样例对应的label。\n",
    "example, label = features['i'], features['j']\n",
    "\n",
    "batch_size = 3\n",
    "capacity = 1000 + 3 * batch_size\n",
    "\n",
    "example_batch, label_batch = tf.train.batch([example, label], batch_size=batch_size, capacity=capacity)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for i in range(2):\n",
    "        cur_example_batch, cur_label_batch = sess.run([example_batch, label_batch])\n",
    "        print(cur_example_batch, cur_label_batch)\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
