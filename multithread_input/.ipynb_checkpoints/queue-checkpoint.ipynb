{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow数据读取机制中内存队列，文件名队列详解见[十图详解tensorflow数据读取机制](https://zhuanlan.zhihu.com/p/27238630)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多线程与queue（内存队列）\n",
    "Tensorflow中提供了FIFOQueue和RandomShuffleQueue两种内存队列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "1\n",
      "11\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 创建一个先进先出队列，指定队列中最多可以保存两个元素，并指定类型\n",
    "q = tf.FIFOQueue(2, dtypes='int32')\n",
    "\n",
    "# 创建一个随机队列，它会将队列中元素打乱，每次出队操作得到的是从当前队列\n",
    "# 所有元素中随机挑选的一个，必须传入min_after_dequeue（队列中最少元素个数）参数\n",
    "# q = tf.RandomShuffleQueue(2, min_after_dequeue=0, dtypes='int32')\n",
    "\n",
    "# 使用enqueue_many初始化函数，使用队列之前必须明确调用初始化\n",
    "init = q.enqueue_many(([0, 10], ))\n",
    "\n",
    "# 使用dequeue函数出队\n",
    "x = q.dequeue()\n",
    "\n",
    "y = x + 1\n",
    "\n",
    "# 重新入队\n",
    "q_inc = q.enqueue([y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 运行初始化队列操作\n",
    "    init.run()\n",
    "    for _ in range(5):\n",
    "        v, _  = sess.run([x, q_inc])\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow提供了**tf.Coordinator**和**tf.QueueRunner**两个类来完成多线程协同的功能。**tf.Coordinator**主要用于协同多个线程一起停止，并提供了**should_stop**，**request_stop**和**join**三个函数。\n",
    "\n",
    "启动线程之前，需要声明一个`tf.Coordinator`类，并将这个类传入每一个创建的建成中。启动的线程需要一直查询`tf.Coordinator`类中提供的`should_stop`函数，当这个函数的返回值为True时，当前线程退出。每一个启动的线程都可以通过调用`request_stop`函数通知其他线程退出。\n",
    "\n",
    "当一个线程调用`request_stop`函数后，`should_stop`函数的返回值被设置为True。这样其他线程就可以同时终止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on id: 1\n",
      "Working on id: 0\n",
      "Working on id: 2\n",
      "\n",
      "\n",
      "Working on id: 3\n",
      "Working on id: 4\n",
      "\n",
      "\n",
      "\n",
      "Working on id: 0\n",
      "\n",
      "Working on id: 1\n",
      "\n",
      "Working on id: 2\n",
      "\n",
      "Working on id: 3\n",
      "\n",
      "Working on id: 4\n",
      "\n",
      "Working on id: 0\n",
      "\n",
      "Working on id: 1\n",
      "\n",
      "Working on id: 2\n",
      "\n",
      "Working on id: 3\n",
      "\n",
      "Working on id: 4\n",
      "\n",
      "Working on id: 0\n",
      "\n",
      "Working on id: 1\n",
      "\n",
      "Working on id: 2\n",
      "\n",
      "Working on id: 3\n",
      "\n",
      "Working on id: 4\n",
      "\n",
      "Working on id: 0\n",
      "\n",
      "Working on id: 1\n",
      "\n",
      "Working on id: 2\n",
      "\n",
      "Working on id: 3\n",
      "\n",
      "Working on id: 4\n",
      "\n",
      "Working on id: 0\n",
      "\n",
      "Working on id: 1\n",
      "\n",
      "Working on id: 2\n",
      "\n",
      "Working on id: 3\n",
      "\n",
      "Working on id: 4\n",
      "\n",
      "Working on id: 0\n",
      "\n",
      "Working on id: 1\n",
      "\n",
      "Stoping from id: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# 线程中运行的程序，该程序每隔1s判断是否需要停止并打印自己的ID\n",
    "def my_loop(coord, worker_id):\n",
    "    # 使用tf.Coordinator类提供的协同工具判断当前线程是否需要停止\n",
    "    while not coord.should_stop():\n",
    "        # 随机停止所有线程\n",
    "        if(np.random.rand() < 0.1):\n",
    "            print('Stoping from id: %d\\n' % worker_id)\n",
    "            # 调用coord.request_stop()通知其他线程停止\n",
    "            coord.request_stop()\n",
    "        else:\n",
    "            # 打印当前线程id\n",
    "            print('Working on id: %d\\n' % worker_id)\n",
    "        time.sleep(1)\n",
    "        \n",
    "# 声明一个tf.train.Coordinator类协同多个线程\n",
    "coord = tf.train.Coordinator()\n",
    "# 创建5个线程\n",
    "threads = [threading.Thread(target=my_loop, args=(coord, i)) for i in range(5)]\n",
    "# 启动所有线程\n",
    "for t in threads:\n",
    "    t.start()\n",
    "\n",
    "# 等待所有线程退出\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.QueueRunner**用于启动多个线程操作同一个队列，启动的这些线程可以通过`tf.Coordinator`统一管理。\n",
    "\n",
    "以下程序利用多线程将随机数入队，单线程出队打印。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67256\n",
      "-0.947661\n",
      "2.3614\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "queue = tf.FIFOQueue(100, 'float')\n",
    "# 定义队列的入队操作，tf.random_normal()从正态分布输出随机值\n",
    "enqueue_op = queue.enqueue([tf.random_normal([1])])\n",
    "\n",
    "# 创建多个线程运行队列的入队操作，queue是被操作的队列，[enqueue_op] * 5表示启动5个\n",
    "# 线程，每个线程中运行的是enqueue_op操作\n",
    "qr = tf.train.QueueRunner(queue, [enqueue_op] * 5)\n",
    "\n",
    "# 将定义过的QueueRunner加入Tensorflow计算图上指定的集合\n",
    "# tf.train.add_queue_runner函数没有指定集合\n",
    "# 则加入默认集合tf.GraphKeys.QUEUE_RUNNERS\n",
    "tf.train.add_queue_runner(qr)\n",
    "# 定义出队操作\n",
    "out_tensor = queue.dequeue()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 使用tf.train.Coordinator协同启动的线程\n",
    "    coord = tf.train.Coordinator()\n",
    "    # 使用tf.train.QueueRunner时，需要明确调用tf.train.start_queue_runners\n",
    "    # 启动所有线程。tf.train.start_queue_runners函数默认启动tf.GraphKeys.QUEUE_RUNNERS\n",
    "    # 集合中所有的QueueRunner。\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    # 获取队列中的取值\n",
    "    for _ in range(3):\n",
    "        print(sess.run(out_tensor)[0])\n",
    "    \n",
    "    # 读取完值后，使用tf.train.Coordinator停止所有线程\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入文件名队列\n",
    "**tf.train.string_input_producer**函数会使用初始化时提供的文件列表创建一个**文件名队列**。\n",
    "\n",
    "通过设置**shuffle**参数，可以打乱文件名队列中的顺序。当`shuffle`参数为True时，文件在加入文件名队列之前就会被打乱顺序。`tf.train.string_input_producer`生成的文件名队列可以同时被多个文件读取线程操作，而且会将队列中的文件均匀地分给不同线程。\n",
    "\n",
    "通过设置**num_epochs**参数限制文件名队列加载文件列表的最大轮数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "data_path = os.path.join('.', 'data')\n",
    "if(not os.path.exists(data_path)):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "# 模拟海量数据情况下将数据写入不同的文件。num_shards定义了总共写入多少个TFRecords文件。\n",
    "# instatnces_per_shard定义了每个TFRcords文件中有多少个数据。\n",
    "num_shards = 2\n",
    "instances_per_shard = 2\n",
    "\n",
    "for i in range(num_shards):\n",
    "    # 将数据分为多个文件时，可以将不同文件以类似0000n-of-0000m的前缀区分，m表示TFRecords数据\n",
    "    # 总个数（2个），n表示第几个TFRecords数据\n",
    "    filename = os.path.join(data_path, 'data_000%d_of_000%d.tfrecords' % (i, num_shards))\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    # 将数据封装为Example结构写入TFRecord文件。这里instances_per_shard=2，意味着每个TFRecords中\n",
    "    # example的数量为2。第一个example中'i'的值为i,'j'的值为0，第二个example中'i'的值为i，\n",
    "    # 'j'的值为1 \n",
    "    for j in range(instances_per_shard):\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'i': tf.train.Feature(int64_list=tf.train.Int64List(value=[i])),\n",
    "            'j': tf.train.Feature(int64_list=tf.train.Int64List(value=[j]))\n",
    "        }))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`tf.train.match_filenames_once`和`tf.train.string_input_producer`读取TFRecrods中的数据。\n",
    "\n",
    "使用`tf.train.match_filenames_once`创建文件名列表读取TFRecords数据时，一定要进行**变量初始化**。\n",
    "\n",
    "```python\n",
    "...\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "   sess.run(init)\n",
    "   ... \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[0, 1]\n",
      "[1, 0]\n",
      "[1, 1]\n",
      "[0, 0]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "data_path = os.path.join('.', 'data')\n",
    "\n",
    "# 要读取train.match_filenames_once中的结果，在run之前需要先调用\n",
    "# global_variables_initializer()和local_variables_initializer()初始化。\n",
    "# train.match_filenames_once中的参数是一个正则表达式pattern，用以匹配文件名，\n",
    "# 例如这里传入的参数是./data/*，会匹配data中所有文件。\n",
    "# 它会返回一个文件名列表，其中每个元素是bytes格式的文件名。\n",
    "filenames = tf.train.match_filenames_once(os.path.join(data_path, '*'))\n",
    "\n",
    "# 如果不给定num_epochs参数值，则会一直生成下去（类似于python generator）\n",
    "filename_queue = tf.train.string_input_producer(filenames, shuffle=False)\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "        'i': tf.FixedLenFeature([], tf.int64),\n",
    "        'j': tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    ")\n",
    "\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    # 多次执行获取数据的操作\n",
    "    for _ in range(6):\n",
    "        # 打印出来的结果，左边一列是sample值，右边一列是label值\n",
    "        print(sess.run([features['i'], features['j']]))\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组合训练数据（batching）\n",
    "Tensorflow提供了**tf.train.batch**和**tf.train.shuffle_batch**函数将单个样例组织成batch形式输出。这两个函数会生成一个**队列**，队列的入队操作是**生成单个sample**的方法，而每次出队得到的是一个**batch**。\n",
    "\n",
    "batching队列同文件名队列一样，使用之前必须**初始化变量**。\n",
    "\n",
    "下面一个cell中展示的时`tf.train.batch`的使用方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1] [0 1 0]\n",
      "[1 0 0] [1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "data_path = os.path.join('.', 'data')\n",
    "\n",
    "# 要读取train.match_filenames_once中的结果，在run之前需要先调用\n",
    "# global_variables_initializer()和local_variables_initializer()初始化。\n",
    "# train.match_filenames_once返回一个文件名列表，其中每个元素是bytes格式的文件名\n",
    "filenames = tf.train.match_filenames_once(os.path.join(data_path, '*'))\n",
    "\n",
    "# 如果不给定num_epochs参数值，则会一直生成下去（类似于python generator）\n",
    "filename_queue = tf.train.string_input_producer(filenames, shuffle=False)\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "        'i': tf.FixedLenFeature([], tf.int64),\n",
    "        'j': tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    ")\n",
    "\n",
    "# ------ 以上是上一节中匹配文件名并创建读取TFRecorder op的代码 ------\n",
    "\n",
    "# 使用上一节中的Example读取到的数据。假设i表示一个sample的特征向量，\n",
    "# 比如一张图像的像素矩阵；j表示该样例对应的label。\n",
    "example, label = features['i'], features['j']\n",
    "\n",
    "batch_size = 3\n",
    "# capacity参数是batch queue中元素个数的最大值\n",
    "capacity = 1000 + 3 * batch_size\n",
    "\n",
    "example_batch, label_batch = tf.train.batch([example, label], batch_size=batch_size, capacity=capacity)\n",
    "\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for i in range(2):\n",
    "        cur_example_batch, cur_label_batch = sess.run([example_batch, label_batch])\n",
    "        # batch为3，一个列表中是3个example值或3个label值\n",
    "        print(cur_example_batch, cur_label_batch)\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "下面一个cell中展示了`tf.train.shuffle_batch`的使用方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "[1 0 0] [1 1 1]\n",
      "[1 1 0] [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "data_path = os.path.join('.', 'data')\n",
    "\n",
    "# 要读取train.match_filenames_once中的结果，在run之前需要先调用\n",
    "# global_variables_initializer()和local_variables_initializer()初始化。\n",
    "# train.match_filenames_once返回一个文件名列表，其中每个元素是bytes格式的文件名\n",
    "filenames = tf.train.match_filenames_once(os.path.join(data_path, '*'))\n",
    "\n",
    "# 如果不给定num_epochs参数值，则会一直生成下去（类似于python generator）\n",
    "filename_queue = tf.train.string_input_producer(filenames, shuffle=False)\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "        'i': tf.FixedLenFeature([], tf.int64),\n",
    "        'j': tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    ")\n",
    "\n",
    "# ------ 以上是上一节中匹配文件名并创建读取TFRecorder op的代码 ------\n",
    "\n",
    "example, label = features['i'], features['j']\n",
    "\n",
    "# tf.train.shuffle_batch中，min_after_dequeue参数是特有的。min_after_dequeue参数限制\n",
    "# 了出队队列中元素的最少个数。当队列中元素太少时，随机打乱sample顺序的作用不大。当出\n",
    "#队函数被调用但是队列中元素不够时，出队操作会等待更多元素入队才会完成。\n",
    "\n",
    "batch_size = 3\n",
    "capacity = 1000 + batch_size * 3\n",
    "\n",
    "example_batch, label_batch = tf.train.shuffle_batch(\n",
    "    [example, label], batch_size=batch_size,\n",
    "    capacity=capacity, min_after_dequeue=30\n",
    ")\n",
    "\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for i in range(2):\n",
    "        cur_example_batch, cur_label_batch = sess.run([example_batch, label_batch])\n",
    "        # batch为3，一个列表中是3个example值或3个label值。可以看到sample和对应label的\n",
    "        # 顺序已经被打乱了\n",
    "        print(cur_example_batch, cur_label_batch)\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.train.batch`和`tf.train.shuffle_batch`也提供了并行化处理输入数据的方法。通过设置`tf.train.shuffle_batch`（`tf.train.batch`同理）中的**num_thread**参数，可以指定多个线程同时执行入队操作。这样多个线程会同时读取**一个文件**中的不同**sample**进行预处理。\n",
    "\n",
    "如果需要多个线程处理**不同文件**中的sample时，可以使用**tf.train.shuffle_batch_join**（不打乱顺序使用**tf.train.batch_join**）。\n",
    "\n",
    "+ **tf.train.shuffle_batch**和**tf.train.shuffle_batch_join**的优劣：\n",
    "\n",
    "   + **tf.train.shuffle_batch**中不同线程会读取同一个文件，如果一个文件中的sample比较相似，那么神经网络训练效果会有影响。所有使用`tf.train.shuffle_batch`前创建TFRecord文件时就要把sample随即打乱。\n",
    "   + **tf.train.shuffle_batch_join**中不同线程会读取不同文件，如果读取的线程数比总文件数还大，那么多个线程会读取同一个文件中相近部分的数据。而且多个线程读取多个文件可能会导致过多的硬盘寻址。从而使读取效率降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入数据处理框架\n",
    "\n",
    "将以上内存队列、文件名队列、batch队列组合处理输入数据。\n",
    "\n",
    "假设TFRecord文件全部放置在`/data/`目录中。TFRecord文件中每个example存储的是一张图片的bytes。格式如下：\n",
    "```python\n",
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'image': tf.train.Feature(int64_list=tf.train.BytesList(value=[image_raw])),\n",
    "    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "    'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "    'width': tf.train.Feature(int64_list=tf.train.Int64List(value=[weight])),\n",
    "    'channels': tf.train.Feature(int64_list=tf.train.Int64List(value=[channels])),\n",
    "}))\n",
    "```\n",
    "\n",
    "**注：案例中没有原始数据，不能直接运行。需要根据以上格式处理源数据并放置到`/data/`目录中方可运行。另外引入了preprocess模块。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import preprocess\n",
    "\n",
    "# TRAINING_THREADS是并行的线程个数\n",
    "TRAINING_THREADS = 8\n",
    "\n",
    "# 创建文件名队列。在调用输入数据处理流程前，需要统一所有原始文件格式并存储到TFRecord\n",
    "# 文件中。并放置到/data/目录中。\n",
    "\n",
    "data_path = os.path.join('.', 'data')\n",
    "\n",
    "filenames = tf.train.match_filenames_once(os.path.join(data_path, '*'))\n",
    "filename_queue = tf.train.string_input_producer(filenames, shuffle=False)\n",
    "\n",
    "# 使用TFRecordReader读取并解析TFRecords数据。\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "        'image': tf.FixedLenFeature([], tf.string),\n",
    "        'label': tf.FixedLenFeature([], tf.int64),\n",
    "        'height': tf.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.FixedLenFeature([], tf.int64),\n",
    "        'channels': tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    ")\n",
    "\n",
    "image, label = features['image'], features['label']\n",
    "height, width = features['height'], features['width']\n",
    "channels = features['channels']\n",
    "\n",
    "# 从原始图像数据解析出像素矩阵，并根据图像尺寸还原图像\n",
    "decode_image = tf.decode_raw(image, tf.uint8)\n",
    "decode_image.set_shape([height, width, channels])\n",
    "\n",
    "# 定义神经网络输入层图片的大小\n",
    "image_size = 299\n",
    "\n",
    "# 图像预处理，调用preprocess.py中的preprocess_for_train函数。\n",
    "distorted_image = preprocess.preprocess_for_train(decoded_image, image_size, image_size, None)\n",
    "\n",
    "# 将处理后的图像和标签数据通过tf.train.shuffle_batch整理成神经网络训练时需要的batch\n",
    "min_after_dequeue = 10000\n",
    "batch_size = 100\n",
    "capacity = min_after_dequeue + 3 * batch_size\n",
    "image_batch, label_batch = tf.train.shuffle_batch(\n",
    "    [distorted_image, label], batch_size=batch_size,\n",
    "    capacity=capacity, min_after_dequeue=min_after_dequeue\n",
    ")\n",
    "\n",
    "# 定义神经网络的结构以及优化过程。image_batch可以作为输入提供给神经网络的输入层。\n",
    "# label_batch则提供了输入batch中对应的label\n",
    "logit = inference(image_batch)\n",
    "loss = calc_loss(logit, label_batch)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# 初始化器\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 创建线程协同器\n",
    "    coord = tf.train.Coordinator()\n",
    "    # 启动多线程\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    # 训练\n",
    "    for i range(TRAINING_THREADS):\n",
    "        sess.run(train_step)\n",
    "    \n",
    "    # 停止所有线程\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
