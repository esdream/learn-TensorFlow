{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(features, batch_size=50, n_epochs=1000):\n",
    "    \"\"\"\n",
    "    Batch generator for the iris dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate batches\n",
    "    for epoch in range(n_epochs):\n",
    "        start_index = 0\n",
    "        while start_index != -1:\n",
    "            # Calculate the end index of the batch to generate\n",
    "            end_index = start_index + batch_size if start_index + batch_size < n else -1\n",
    "            yield features[start_index:end_index]\n",
    "            \n",
    "# Auto Encoder\n",
    "class TF_AutoEncoder:\n",
    "    def __init__(self, features, labels, dtype=tf.float32)\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.encoder = dict()\n",
    "        \n",
    "    def fit(self, n_dimensions):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "\n",
    "            # Input variable\n",
    "            X = tf.placeholder(self.dtype, shape=(None, self.features.shape[1]))\n",
    "\n",
    "            # Network variables\n",
    "            encoder_weights = tf.Variable(tf.random_normal(shape=(self.features.shape[1], n_dimensions)))\n",
    "            encoder_bias = tf.Variable(tf.zero(shape=[n_dimensions]))\n",
    "\n",
    "            decoder_weights = tf.Variable(tf.random_normal(shape=(n_dimensions, self.features.shape[1])))\n",
    "            decoder_bias = tf.Variable(tf.zeros(shape=[self.features.shape[1]]))\n",
    "\n",
    "            # Encoder part\n",
    "            encoding = tf.nn.sigmoid(tf.nn.sigmoid(tf.add(tf.matmul(X, encoding_weights))))\n",
    "\n",
    "            # Decoder part\n",
    "            predicted_x = tf.nn.sigmoid(tf.add(tf.matmul(encoding, decoder_weights), encoder_bias))\n",
    "\n",
    "            # Define the cost function and optimizer to minimize squared error\n",
    "            cost = tf.reduce_mean(tf.pow(tf.subtract(predicted_x, X), 2))\n",
    "            optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            # Initialize global variables\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for batch_x in batch_generator(self.features):\n",
    "                \n",
    "                self.encoder['weights'], self.encoder['bias'], _ = session.run([encoder_weights, encoder_bias, optimier], feed_dict={X: batch_x})\n",
    "                \n",
    "        def reduce(self):\n",
    "            return np.add(np.matmul(self.features, self.encoder['weights']), self.encoder['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_dataset = datasets.load_iris()\n",
    "\n",
    "# Mix the data befor trainning\n",
    "n = len(iris_dataset.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
